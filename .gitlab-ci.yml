stages:
  - build
  - publish
  - package
  - shared-pipeline
  - publish
  - benchmarks

include:
  - local: .gitlab/one-pipeline.locked.yaml

variables:
  DOTNET_PACKAGE_VERSION:
    description: "Used by the package stage when triggered manually"
  REPO_NOTIFICATION_CHANNEL: "#apm-dotnet-bots"

build:
  except:
    variables:
      - $DEPLOY_TO_REL_ENV == "true"
      - $CI_COMMIT_TAG # We don't need to build/publish when building a release tag
  stage: build
  timeout: 2h
  tags: ["windows-v2:2019"]
  hooks:
    pre_get_sources_script:
      - git config --system core.longpaths true
  script:
    - if (Test-Path build-out) { remove-item -recurse -force build-out }
    - if (Test-Path artifacts-out) { remove-item -recurse -force artifacts-out }
    - |
      docker run --rm -m 10240M `
        -v "$(Get-Location):c:\mnt" `
        -e CI_JOB_ID=${CI_JOB_ID} `
        -e ENABLE_MULTIPROCESSOR_COMPILATION=false `
        -e WINDOWS_BUILDER=true `
        -e AWS_NETWORKING=true `
        -e SIGN_WINDOWS=true `
        -e NUGET_CERT_REVOCATION_MODE=offline `
        registry.ddbuild.io/images/mirror/datadog/dd-trace-dotnet-docker-build:dotnet10 `
        Info Clean BuildTracerHome BuildProfilerHome BuildNativeLoader BuildDdDotnet PublishFleetInstaller PackageTracerHome ZipSymbols SignDlls SignMsi DownloadWinSsiTelemetryForwarder
    - mkdir artifacts-out
    - xcopy /e/s build-out\${CI_JOB_ID}\*.* artifacts-out
    - remove-item -recurse -force build-out\${CI_JOB_ID}
    - get-childitem build-out
    - get-childitem artifacts-out
  artifacts:
    expire_in: 2 weeks
    paths:
    - artifacts-out

publish:
  only:
    - master
    - main
    - /^hotfix.*$/
    - /^release.*$/
  except:
    variables:
      - $DEPLOY_TO_REL_ENV == "true"
      - $CI_COMMIT_TAG # We don't need to build/publish when building a release tag
  stage: publish
  tags: ["windows-v2:2019"]
  dependencies:
    - build
  hooks:
    pre_get_sources_script:
      - git config --system core.longpaths true
  script:
    - $result =  aws sts assume-role --role-arn "arn:aws:iam::486234852809:role/ci-datadog-windows-filter" --role-session-name AWSCLI-Session
    - $resultjson = $result | convertfrom-json
    - $credentials = $($resultjson.Credentials)
    - $Env:AWS_ACCESS_KEY_ID="$($credentials.AccessKeyId)"
    - $Env:AWS_SECRET_ACCESS_KEY="$($credentials.SecretAccessKey)"
    - $Env:AWS_SESSION_TOKEN="$($credentials.SessionToken)"
    - |
      $i = 0
      do {
          try {
              # The grants option at the end is used to allow public access on the files we upload as the acls only aren't enough.
              aws s3 cp artifacts-out/ s3://dd-windowsfilter/builds/tracer/${CI_COMMIT_SHA} --recursive --region us-east-1 --exclude "*" --include "*.zip" --include "*.msi" --include "telemetry_forwarder.exe" --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers full=id=3a6e02b08553fd157ae3fb918945dd1eaae5a1aa818940381ef07a430cf25732
              If ($LASTEXITCODE -eq 0) { 
                return
              }

              throw "Error uploading artifacts to S3"
          } catch {
              $msg = $Error[0].Exception.Message
              Write-Output "Encountered error during while publishing to S3. Error Message is $msg."
              Write-Output "Retrying..."
              $i++
              Start-Sleep -Milliseconds 100
          }
      } while ($i -lt 3)
      # If we got here, all retries failed â€“ fail the job:
      Write-Error "Failed to upload artifacts to S3 after $i attempts."
      exit 1


download-single-step-artifacts:
  stage: package
  image: registry.ddbuild.io/docker:20.10.13-gbi-focal
  timeout: 45m
  tags: [ "arch:amd64" ]
  needs:
    - job: build
      optional: true
      artifacts: true
  rules:
    - if: $DOTNET_PACKAGE_VERSION
      when: on_success
    - if: '$CI_COMMIT_TAG =~ /^v[0-9]+\.[0-9]+\.[0-9]+(-prerelease)?$/'
      when: on_success # Artifacts are downloaded from the GitHub release, which creates the tag on publish
      allow_failure: false
    - when: on_success # Artifacts come from Azure pipeline, but as we already depend on build, we already have a delayed start
  script:
    - .gitlab/download-single-step-artifacts.sh
    - cp tracer/build/artifacts/requirements.json artifacts/requirements.json
    - cp -r artifacts-out artifacts/windows
  artifacts:
    expire_in: 2 weeks
    paths:
      - artifacts

requirements_json_test:
  rules:
    - when: on_success
  variables:
    REQUIREMENTS_BLOCK_JSON_PATH: "tracer/build/artifacts/requirements_block.json"
    REQUIREMENTS_ALLOW_JSON_PATH: "tracer/build/artifacts/requirements_allow.json"

package-oci:
  needs: [ download-single-step-artifacts ]

generate-lib-init-pinned-tag-values:
  variables:
    ADDITIONAL_TAG_SUFFIXES: musl #add -musl to all generated image tags

configure_system_tests:
  variables:
    SYSTEM_TESTS_SCENARIOS_GROUPS: "simple_onboarding,simple_onboarding_profiling,simple_onboarding_appsec,lib-injection"

deploy_to_reliability_env:
  rules:
    - when: never # dd-trace-dotnet does not use reliability environment

download-serverless-artifacts:
  stage: package
  image: registry.ddbuild.io/docker:20.10.13-gbi-focal
  tags: [ "arch:amd64" ]
  needs: []
  rules:
    - if: $DOTNET_PACKAGE_VERSION
      when: on_success
    - if: '$CI_COMMIT_TAG =~ /^v[0-9]+\.[0-9]+\.[0-9]+(-prerelease)?$/'
      when: on_success # Artifacts are downloaded from the GitHub release, which creates the tag on publish
      allow_failure: false
    - when: delayed # Artifacts come from Azure pipeline, wait a reasonable time before polling
      start_in: 15 minutes
  script:
    - .gitlab/download-serverless-artifacts.sh
  artifacts:
    expire_in: 2 weeks
    paths:
      - artifacts

aws-lambda-layer:
  stage: package
  needs: [download-serverless-artifacts]
  when: manual
  trigger:
    project: DataDog/dd-trace-dotnet-aws-lambda-layer
    strategy: depend
  variables:
    UPSTREAM_PIPELINE_ID: $CI_PIPELINE_ID
    UPSTREAM_PIPELINE_REF: $CI_COMMIT_REF_NAME
  allow_failure: true

benchmark-serverless:
  stage: benchmarks
  image: registry.ddbuild.io/ci/serverless-tools:1
  tags: ["arch:amd64"]
  when: on_success
  needs:
    - benchmark-serverless-trigger
  script:
    - git clone https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/serverless-tools.git ./serverless-tools && cd ./serverless-tools
    - ./ci/check_trigger_status.sh

benchmark-serverless-trigger:
  stage: benchmarks
  needs: []
  trigger:
    project: DataDog/serverless-tools
    strategy: depend
  allow_failure: true
  variables:
    UPSTREAM_PIPELINE_ID: $CI_PIPELINE_ID
    UPSTREAM_PROJECT_URL: $CI_PROJECT_URL
    UPSTREAM_COMMIT_BRANCH: $CI_COMMIT_BRANCH
    UPSTREAM_COMMIT_AUTHOR: $CI_COMMIT_AUTHOR
    UPSTREAM_COMMIT_TITLE: $CI_COMMIT_TITLE
    UPSTREAM_COMMIT_TAG: $CI_COMMIT_TAG
    UPSTREAM_PROJECT_NAME: $CI_PROJECT_NAME
    UPSTREAM_GITLAB_USER_LOGIN: $GITLAB_USER_LOGIN
    UPSTREAM_GITLAB_USER_EMAIL: $GITLAB_USER_EMAIL

macrobenchmarks:
  stage: benchmarks
  needs: []
  trigger:
    include: .gitlab/benchmarks/macrobenchmarks.yml
  allow_failure: true
  rules:
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == "master"'
      when: always
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $BENCHMARK_RUN == "true"'
      when: always
    - when: manual

microbenchmarks:
  stage: benchmarks
  needs: []
  trigger:
    include: .gitlab/benchmarks/microbenchmarks.yml
    strategy: depend
  allow_failure: true

dsm_throughput:
  stage: benchmarks
  trigger:
    include: .gitlab/benchmarks/dsm-throughput.yml
  allow_failure: true
  rules:
    - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_BRANCH == "master"'
      when: always
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $BENCHMARK_RUN == "true"'
      when: always
    - when: manual


validate_supported_configurations_local_file:
  stage: build
  rules:
    - when: on_success
  extends: .validate_supported_configurations_local_file
  variables:
    LOCAL_JSON_PATH: "tracer/src/Datadog.Trace/Configuration/supported-configurations.json"