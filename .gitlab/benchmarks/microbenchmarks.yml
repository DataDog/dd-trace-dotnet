.dd-octo-sts-setup:
  before_script:
    - |
      set +e
      echo "Attempting to retrieve a GitHub token for scope '$DDOCTOSTS_SCOPE' with policy '$DDOCTOSTS_POLICY' with dd-octo-sts..."
      error_output=$({ dd-octo-sts token --scope $DDOCTOSTS_SCOPE --policy $DDOCTOSTS_POLICY > "/tmp/github-token"; } 2>&1)
      exit_code=$?
      if [ $exit_code -ne 0 ]; then
        echo "Error: Failed to retrieve GitHub token."
        echo "Original error: $error_output"
        echo "Continuing execution anyway..."
      fi
      set -e

stages:
  - build
  - benchmarks
  - gate

workflow:
  auto_cancel:
    on_new_commit: interruptible

build-dd-trace-dotnet-microbenchmarks-ami:
  stage: build
  tags: ["arch:amd64"]
  timeout: 3h
  allow_failure: true
  when: manual
  image: registry.ddbuild.io/images/benchmarking-platform-tools-ubuntu:dd-trace-dotnet-micro
  id_tokens:
    DDOCTOSTS_ID_TOKEN:
      aud: dd-octo-sts
  variables:
    # Allows ephemeral instances to read content from dd-trace-dotnet
    # This is not strictly necessary in the current AMI build
    DDOCTOSTS_SCOPE: "DataDog/dd-trace-dotnet"
    DDOCTOSTS_POLICY: "gitlab.github-access.read-contents"

    AWS_REGION: "us-east-1"

    # Branch containing a provision for building the AMI
    BP_INFRA_BENCHMARKING_PLATFORM_BRANCH: "dd-trace-dotnet/micro"

    PROVISION_FILE: "platform/ephemeral-infra/ami.yaml"

    # Where AMI creation artifacts will be stored
    BP_INFRA_ARTIFACTS_BUCKET_NAME: "windows-benchmarking-results-us-east-1"

    # Whether to cleanup instances after building the AMI, since the AMI is 
    # based on an instance that is created in this job
    CLEANUP: "true"
  before_script:
    - !reference [.dd-octo-sts-setup, before_script]
  script:
    - git clone --branch $BP_INFRA_BENCHMARKING_PLATFORM_BRANCH https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/benchmarking-platform platform
    - echo "GITHUB_TOKEN=$(cat /tmp/github-token)" > .env
    - CLEANUP_ARG=$([[ "$CLEANUP" == "false" ]] && echo "--no-cleanup" || echo "")
    - |
      bp-infra launch --region "${AWS_REGION}" --os "windows" \
        --provision "${PROVISION_FILE}" \
        --bypass-stack-destroy \
        --env .env \
        $CLEANUP_ARG
  after_script:
    # Makes sure the instance is cleaned up.
    # Note: This does not clean up the created AMI.
    - |
      if [ "$CLEANUP" == "true" ]; then
        bp-infra cleanup --region "${AWS_REGION}" --os "windows" \
          --provision "${PROVISION_FILE}" \
          --bypass-stack-destroy
      else 
        echo "'CLEANUP' is set to 'false'. Will not cleanup."
      fi

run-benchmarks:
  stage: benchmarks
  tags: ["arch:amd64"]
  timeout: 2h
  # Image created in the following job https://gitlab.ddbuild.io/DataDog/benchmarking-platform-tools/-/jobs/869830045
  image: registry.ddbuild.io/images/benchmarking-platform-tools-ubuntu:dd-trace-dotnet-micro
  id_tokens:
    DDOCTOSTS_ID_TOKEN:
      aud: dd-octo-sts
  rules:
    - if: $CI_COMMIT_REF_NAME =~ /^v[0-9]+\.[0-9]+\.[0-9]+(-prerelease)?$/
      when: never
    - if: $CI_COMMIT_REF_NAME == "master"
      interruptible: false
    - interruptible: true
  artifacts:
    name: "artifacts"
    when: always
    paths:
      - reports/
    expire_in: 3 months
  variables:
    # Allows ephemeral instances to read content from benchmarking-platform
    DDOCTOSTS_SCOPE: "DataDog/benchmarking-platform"
    DDOCTOSTS_POLICY: "gitlab.github-access.read-contents"

    AWS_REGION: "us-east-1"

    # Branch containing 1. scripts to launch Windows benchmarks on ephemeral 
    # instances (to be used by GitLab CI runners) and 2. scripts to run Windows 
    # benchmarks (to be used by the ephemeral instances).
    BP_INFRA_BENCHMARKING_PLATFORM_BRANCH: "dd-trace-dotnet/micro"

    # Where benchmarking results will be stored
    BP_INFRA_ARTIFACTS_BUCKET_NAME: "windows-benchmarking-results-us-east-1"

    # Whether to mock benchmark execution by simply copying the latest 
    # master results. Useful for testing the after_script steps
    BP_INFRA_TEST: "false"

    # Whether to cleanup ephemeral instances after benchmarks are run
    CLEANUP: "true"

    # Where to look for benchmarking artifacts for uploading to the BP UI
    ARTIFACTS_DIR: "reports"
  before_script:
    - !reference [.dd-octo-sts-setup, before_script]
  script:
    - export GITHUB_TOKEN=$(cat /tmp/github-token)
    - git clone --branch $BP_INFRA_BENCHMARKING_PLATFORM_BRANCH https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/benchmarking-platform platform
    - mkdir -p reports
    - ./platform/steps/run-windows-benchmarks.sh
  after_script:
    - |
      if [ "$CLEANUP" == "true" ]; then
        bp-infra cleanup --provision ./platform/ephemeral-infra/instance.yaml \
          --region "${AWS_REGION}" \
          --bypass-stack-destroy
      else
        echo "'CLEANUP' is set to 'false'. Will not cleanup."
      fi
    - ./platform/steps/fetch-results.sh
    - ./platform/steps/analyze-results.sh
    - ./platform/steps/post-pr-comment.sh
    - ./platform/steps/upload-to-bp-ui.sh

# This repository is using PR-level performance quality gates.
# Verify that the check-big-regressions CI job has passed. If any regression happened, merging this PR will be blocked.
# If bypassing is necessary, see https://datadoghq.atlassian.net/wiki/x/8YFzMwE for more details.
.check-big-regressions:
  stage: gate
  tags: ["arch:amd64"]
  image: registry.ddbuild.io/images/benchmarking-platform-tools-ubuntu:latest
  needs: ["run-benchmarks"]
  # We set when: always when applicable to ensure the job runs even if the run-benchmarks job fails.
  rules:
    - if: $CI_COMMIT_REF_NAME =~ /^v[0-9]+\.[0-9]+\.[0-9]+(-prerelease)?$/
      when: never
    - if: $CI_COMMIT_REF_NAME == "master"
      interruptible: false
      when: always
    - interruptible: true
      when: always
  artifacts:
    name: "artifacts"
    when: always
    paths:
      - reports/
    expire_in: "30 days"
  variables:
    BP_INFRA_BENCHMARKING_PLATFORM_BRANCH: "dd-trace-dotnet/micro"

    # Regex to ignore benchmarks identified as unstable
    IGNORED_BENCHMARKS_REGEX: "Trace.Iast.StringAspectsBenchmark|Trace.CharSliceBenchmark|Trace.Asm.AppSecWafBenchmark|Trace.Asm.AppSecBodyBenchmark|Trace.CIVisibilityProtocolWriterBenchmark"
  script: |
    export ARTIFACTS_DIR="$(pwd)/reports"

    if [ -n "${IGNORED_BENCHMARKS_REGEX}" ]; then
      echo "Adding the 'ignored.' prefix to benchmarks matching IGNORED_BENCHMARKS_REGEX:"
      echo "    ${IGNORED_BENCHMARKS_REGEX}"
      for file in "${ARTIFACTS_DIR}"/*.converted.json; do
        file_basename=$(basename "${file}")
        if [[ "${file_basename}" =~ ${IGNORED_BENCHMARKS_REGEX} ]]; then
          echo "    ${file_basename} -> ignored.${file_basename}"
          mv "${file}" "${ARTIFACTS_DIR}/ignored.${file_basename}"
        fi
      done
    fi

    git clone --branch $BP_INFRA_BENCHMARKING_PLATFORM_BRANCH https://gitlab-ci-token:${CI_JOB_TOKEN}@gitlab.ddbuild.io/DataDog/benchmarking-platform /platform
    bp-runner /platform/bp-runner.fail-on-regression.yml --debug

check-big-regressions:
  extends: .check-big-regressions

# TODO: Remove once the decision to consider or ignore benchmarks identified as 
# unstable has been made
check-big-regressions-no-ignored-benchmarks:
  extends: .check-big-regressions
  allow_failure: true  # Allowed to fail since we do not want it to block PRs 
  variables:
    IGNORED_BENCHMARKS_REGEX: ""
